{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Class 3 - clustering and Bayesian learning\n",
    "\n",
    "The examples and exercises in this computer class introduce the student to clustering techniques and Bayesian learning. This computer class can be used in conjunction with chapters 4 and 5 of the reader.\n",
    "\n",
    "*Authors: Cees Diks and Bram Wouters, Faculty Economics and Business, University of Amsterdam (UvA)* <br>\n",
    "*Copyright (C): UvA (2023)* <br>\n",
    "*Credits: some of the examples and formulations are taken from Hastie et al. (2009)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import laplace\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: K-means clustering for California housing market\n",
    "In this section, we will use K-means clustering to segmet geographically the California housing market.\n",
    "\n",
    "In the cell below, we import a dataset about the California housing market from scikit-learn. It contains information about 20640 properties. We will be mostly interested in the geographical information (the columns `Latitude` and `Longitude`) of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "df = california_housing.frame\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** in the cell below, we put the longitude and latitude information of the DataFrame into a single 2-dimensional NumPy array. This is because we prefer to write the K-means clustering algorithm with NumPy.\n",
    "\n",
    "We also plotted the geographical information as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[['Longitude', 'Latitude']])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.scatter(X[:,0], X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** write a function `K_means` that performs the K-means clustering algorithm, given as input the location data `X` and the number of clusters `K`. Use the squared Euclidian distance (in units of longitude and latitude) as the similarity metric. Abort the iterative algorithm if the average squared Euclidian distance with respect to the respective class means does not decrease more than 0.1%, as compared to the previous iteration.\n",
    "\n",
    "The function `K_means` should (at least) return a 1-dimensional array of integers between $0$ and $K-1,$ representing the cluster to which each of the houses is assigned to.\n",
    "\n",
    "Hint: this is not an easy algorithm. It may be useful to first define a number of other functions (subroutines) that perform simple tasks and finally combine these simple functions into the `K_means` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** if you have defined the final clustering of the K-means algorithm as a 1-dimensional array `Y` and you run the cell below, the clustering will be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: Y\n",
    "except:\n",
    "    K = 10\n",
    "    Y = np.random.randint(K, size=X.shape[0])\n",
    "    \n",
    "df['clusters'] = Y\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "\n",
    "ax.scatter(df['Longitude'], df['Latitude'], c=df['clusters']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** define a function `K_means_scan` that runs the K-means algorithm several times (every time with a different initial random segmentation) and return the optimal segmentation. By optimal, we mean the segmentation with the smallest average squared Euclidian distance with respect to the respective class means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** if you have defined the optimal clustering of the K-means algorithm as a 1-dimensional array `Y_best` and you run the cell below, the optimal clustering will be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: Y_best\n",
    "except:\n",
    "    K = 10\n",
    "    Y_best = np.random.randint(K, size=X.shape[0])\n",
    "    \n",
    "df['clusters'] = Y_best\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "\n",
    "ax.scatter(df['Longitude'], df['Latitude'], c=df['clusters']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Bayesian parameter estimation\n",
    "In this section, we consider a sample of size $n$ from the Laplace distribution (also called \"double exponential\" distribution), whose pdf is\n",
    "$$ f(x; \\theta, \\eta) = \\frac{1}{2\\theta} \\exp \\left( - | x - \\eta |/\\theta \\right), $$\n",
    "where $\\theta>0$ is a scale parameter and $\\eta \\in \\mathbb{R}$ is a location parameter. We will assume that $\\theta=1.0$ is a known parameter, while $\\eta$ is unknown.\n",
    "\n",
    "The function `generate_data` in the cell below can be used to generate samples drawn from the Laplace distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, eta, theta):\n",
    "    \"\"\"\n",
    "    Generates a sample of size n drawn from the Laplace distribution with parameters eta (location)\n",
    "    and theta (scale). Returns the sample as a 1-dimensional array.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.random.laplace(loc = eta, scale = theta, size=(n,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** in the cell below, 3 functions are defined. The first function `posterior` computes, based on the data and a prior, the unnormalized posterior for the unknown parameter `eta`. The latter two functions use the function `posterior` to create a normalized posterior that can be applied to an array of input values.\n",
    "\n",
    "Finish the code of the function `posterior`. Use the docstring for details about what the function should do. The latter two functions can remain untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(eta, data, theta = 1.0, prior=None):\n",
    "    \"\"\"\n",
    "    Returns the value (a float) of the unnormalized posterior for a given value of the parameter eta, \n",
    "    based on a sample of observations called data. \n",
    "    \n",
    "    The prior input should be a function with one argument (the parameter eta). If prior is None,\n",
    "    we use a constant prior of value 1.\n",
    "    \"\"\"  \n",
    "    \n",
    "def posterior_function(etas, data, theta = 1.0, prior=None):\n",
    "    \"\"\"\n",
    "    Computes the unnormalized posterior for an array of values for eta (etas).\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array(list(map(lambda x: posterior(x, data, theta = theta, prior=prior), etas)))\n",
    "\n",
    "def posterior_function_normalized(etas, data, N = 1000, left=None, right=None, eta_axis=None, theta=1.0, prior=None):\n",
    "    \"\"\"\n",
    "    Computes the normalized posterior for an array of values for eta (etas).\n",
    "    \"\"\"\n",
    "    \n",
    "    if left is None:\n",
    "        left = np.mean(data) - 5 * theta / np.sqrt(n)\n",
    "    if right is None:\n",
    "        right = np.mean(data) + 5 * theta / np.sqrt(n)\n",
    "    if eta_axis is None:\n",
    "        eta_axis = np.linspace(left, right, N)\n",
    "    \n",
    "    norm = np.sum(posterior_function(eta_axis, data, theta=theta, prior=prior)) * (right - left) / N\n",
    "\n",
    "    return posterior_function(etas, data, theta=theta, prior=prior)/norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** If you completed the previous exercise correctly, the cell below plots two normalized posteriors for `eta` based on `n` datapoints: one for a constant prior, and one for a standard normal prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "eta = 2.2\n",
    "theta = 1.0\n",
    "data = generate_data(n, eta, theta)\n",
    "\n",
    "def standard_normal_prior(eta):\n",
    "    \"\"\"\n",
    "    Defines the standard normal Gaussian as a prior for the unknown parameter eta.\n",
    "    \"\"\"\n",
    "    return np.exp(- eta**2/2) / np.sqrt(2*np.pi)\n",
    "\n",
    "left = np.mean(data) - 5 * theta / np.sqrt(n)\n",
    "right = np.mean(data) + 5 * theta / np.sqrt(n)\n",
    "\n",
    "N = 1000\n",
    "eta_axis = np.linspace(left, right, N)\n",
    "\n",
    "try:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    Lfunc = posterior_function_normalized(eta_axis, data)\n",
    "    ax.plot(eta_axis, Lfunc, label=\"constant prior\");\n",
    "\n",
    "    Lfunc = posterior_function_normalized(eta_axis, data, prior=standard_normal_prior)\n",
    "    ax.plot(eta_axis, Lfunc, label=\"standard normal prior\");\n",
    "\n",
    "    ax.set_title(\"Posterior of $\\eta$\")\n",
    "    ax.set_xlabel(\"$\\eta$\")\n",
    "    ax.legend();\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** plot the normalized posterior for different values of the sample size $n$ (take $n=10,20,40,80,160,260,400$) in the sample graph. Make sure that you understand the trend visible in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** write a function `Bayes_estimator_eta` that returns a Bayes estimate of the unknown parameter $\\eta$, given some data `data`. Make a distinction between two losses: for a `squared` loss (the default) the estimate is the conditional mean of $\\eta$ (in other words: the posterior mean of $\\eta$), while for an `absolute` loss the estimate is the median of the posterior distribution. The function should work correctly for both choices of Bayes estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_estimator_eta(data, loss = \"squared\", N = 1000, theta=1.0):\n",
    "    \"\"\"\n",
    "    Returns the Bayes estimate for the unknown parameter eta. In the case of a squared loss, this is the\n",
    "    posterior mean of eta. In the case of an absolute loss, this is the median of the posterior.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** print for different values of the sample size $n$ (take $n=10,20,40,80,160,260,400$) three different estimates of the unknown parameter $\\eta$:\n",
    "* the sample mean.\n",
    "* the Bayes estimator using a squared loss.\n",
    "* the Bayes estimator using an absolute loss.\n",
    "Observe that all estimators perform better for larger sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
